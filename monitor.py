#!/usr/bin/env python3
"""
ç¾è§‚çš„è®­ç»ƒå®æ—¶ç›‘æ§è„šæœ¬
æ¯éš”10ä¸ªstepå±•ç¤ºæ¨¡å‹å›å¤å’ŒæŒ‡æ ‡åˆ†æ
"""
import json
import sys
import time
import os
from datetime import datetime
from pathlib import Path

# é¢œè‰²ä»£ç 
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'

def print_header(title):
    """æ‰“å°å¤§æ ‡é¢˜"""
    width = 80
    print(f"\n{Colors.HEADER}{'='*width}{Colors.END}")
    print(f"{Colors.HEADER}{Colors.BOLD}{title.center(width)}{Colors.END}")
    print(f"{Colors.HEADER}{'='*width}{Colors.END}\n")

def print_section(title):
    """æ‰“å°å°èŠ‚æ ‡é¢˜"""
    print(f"\n{Colors.CYAN}{Colors.BOLD}â–¶ {title}{Colors.END}")
    print(f"{Colors.CYAN}{'â”€'*70}{Colors.END}")

def print_metric(name, value, unit=""):
    """æ‰“å°å•ä¸ªæŒ‡æ ‡"""
    # æ ¹æ®æ•°å€¼é€‰æ‹©é¢œè‰²
    if isinstance(value, (int, float)):
        if value >= 0.7:
            color = Colors.GREEN
        elif value >= 0.3:
            color = Colors.YELLOW
        else:
            color = Colors.RED
        value_str = f"{value:.4f}{unit}"
    else:
        color = Colors.BLUE
        value_str = f"{value}{unit}"
    
    print(f"  {Colors.BOLD}{name:25}{Colors.END} {color}{value_str}{Colors.END}")

def analyze_metrics(metrics):
    """åˆ†ææŒ‡æ ‡å¹¶è¿”å›è¯Šæ–­"""
    analysis = []
    
    mean_reward = metrics.get("mean_reward", 0)
    parse_rate = metrics.get("parse_rate", 0)
    format_rate = metrics.get("format_rate", 0)
    
    # å¥–åŠ±åˆ†æ
    if mean_reward > 0.5:
        analysis.append(("å¥–åŠ±è‰¯å¥½", Colors.GREEN, f"avg: {mean_reward:.2f}"))
    elif mean_reward > 0:
        analysis.append(("å¥–åŠ±åä½", Colors.YELLOW, f"avg: {mean_reward:.2f}"))
    else:
        analysis.append(("âš ï¸ å¥–åŠ±è¿‡ä½", Colors.RED, f"avg: {mean_reward:.2f} - éœ€æ£€æŸ¥å¥–åŠ±å‡½æ•°"))
    
    # è§£æç‡åˆ†æ
    if parse_rate > 0.8:
        analysis.append(("è§£æç‡é«˜", Colors.GREEN, f"{parse_rate:.1%}"))
    elif parse_rate > 0.5:
        analysis.append(("è§£æç‡ä¸­ç­‰", Colors.YELLOW, f"{parse_rate:.1%}"))
    else:
        analysis.append(("âš ï¸ è§£æç‡ä½", Colors.RED, f"{parse_rate:.1%} - æ¨¡å‹æœªå­¦ä¼šæ ¼å¼"))
    
    # æ ¼å¼ç‡åˆ†æ
    if format_rate > 0.8:
        analysis.append(("æ ¼å¼æ­£ç¡®ç‡é«˜", Colors.GREEN, f"{format_rate:.1%}"))
    elif format_rate > 0.5:
        analysis.append(("æ ¼å¼ç‡ä¸­ç­‰", Colors.YELLOW, f"{format_rate:.1%}"))
    else:
        analysis.append(("âš ï¸ æ ¼å¼ç‡ä½", Colors.RED, f"{format_rate:.1%}"))
    
    # KLåˆ†æ
    kl = metrics.get("approx_kl")
    if kl is not None:
        if kl > 0.5:
            analysis.append(("âš ï¸ KLè¿‡é«˜", Colors.RED, f"{kl:.3f} - ç­–ç•¥åç¦»è¿‡å¤§"))
        elif kl > 0.1:
            analysis.append(("KLæ­£å¸¸", Colors.GREEN, f"{kl:.3f}"))
        else:
            analysis.append(("KLè¾ƒä½", Colors.YELLOW, f"{kl:.3f}"))
    
    return analysis

def print_metrics_dashboard(metrics):
    """æ‰“å°æŒ‡æ ‡ä»ªè¡¨ç›˜"""
    print_section("ğŸ“Š è®­ç»ƒæŒ‡æ ‡")
    
    # æ ¸å¿ƒæŒ‡æ ‡
    print(f"\n{Colors.YELLOW}æ ¸å¿ƒæŒ‡æ ‡:{Colors.END}")
    core_metrics = [
        ("å¹³å‡å¥–åŠ±", metrics.get("mean_reward", 0), ""),
        ("å¥–åŠ±æ ‡å‡†å·®", metrics.get("std_reward", 0), ""),
        ("è§£ææˆåŠŸç‡", metrics.get("parse_rate", 0), ""),
        ("æ ¼å¼æ­£ç¡®ç‡", metrics.get("format_rate", 0), ""),
    ]
    for name, value, unit in core_metrics:
        print_metric(name, value, unit)
    
    # è®­ç»ƒæŒ‡æ ‡
    print(f"\n{Colors.YELLOW}è®­ç»ƒæŒ‡æ ‡:{Colors.END}")
    train_metrics = [
        ("ç­–ç•¥æŸå¤±", metrics.get("policy_loss", 0), ""),
        ("ä»·å€¼æŸå¤±", metrics.get("value_loss", 0), ""),
        ("æ€»æŸå¤±", metrics.get("total_loss", 0), ""),
        ("KLæ•£åº¦", metrics.get("approx_kl", 0), ""),
        ("ç†µ", metrics.get("entropy", 0), ""),
        ("Clipæ¯”ä¾‹", metrics.get("clip_frac", 0), ""),
    ]
    for name, value, unit in train_metrics:
        if value is not None:
            print_metric(name, value, unit)
    
    # æ•ˆç‡æŒ‡æ ‡
    print(f"\n{Colors.YELLOW}æ•ˆç‡æŒ‡æ ‡:{Colors.END}")
    eff_metrics = [
        ("ç”Ÿæˆtoken/ç§’", metrics.get("gen_tokens_per_sec", 0), ""),
        ("æ ·æœ¬/ç§’", metrics.get("samples_per_sec", 0), ""),
        ("GPUæ˜¾å­˜", metrics.get("gpu_mem_gb", 0), " GB"),
        ("å›å¤é•¿åº¦", metrics.get("response_len", 0), ""),
    ]
    for name, value, unit in eff_metrics:
        if value is not None:
            print_metric(name, value, unit)

def print_sample_analysis(sample, idx):
    """ç¾è§‚åœ°æ‰“å°æ ·æœ¬åˆ†æ"""
    print_section(f"æ ·æœ¬ #{idx + 1}")
    
    question = sample.get("question", "")
    response = sample.get("response", "")
    pred = sample.get("pred", "")
    gold = sample.get("gold", "")
    reward = sample.get("reward", 0)
    
    # æ‰“å°é—®é¢˜
    if len(question) > 80:
        question = question[:77] + "..."
    print(f"{Colors.YELLOW}é—®é¢˜:{Colors.END} {question}\n")
    
    # åˆ¤æ–­çŠ¶æ€
    is_correct = pred == gold
    is_parsed = pred is not None and pred != ""
    
    # çŠ¶æ€æ ‡ç­¾
    if is_correct:
        status = f"{Colors.GREEN}{Colors.BOLD}âœ“ å›ç­”æ­£ç¡®{Colors.END}"
    elif is_parsed:
        status = f"{Colors.YELLOW}{Colors.BOLD}âš  è§£ææˆåŠŸä½†é”™è¯¯{Colors.END}"
    else:
        status = f"{Colors.RED}{Colors.BOLD}âœ— è§£æå¤±è´¥{Colors.END}"
    
    print(f"çŠ¶æ€: {status}")
    print(f"é¢„æµ‹: {Colors.BLUE}{pred or 'N/A'}{Colors.END}")
    print(f"ç­”æ¡ˆ: {Colors.GREEN}{gold or 'N/A'}{Colors.END}")
    
    # å¥–åŠ±
    reward_color = Colors.GREEN if reward > 0.5 else (Colors.YELLOW if reward > -0.3 else Colors.RED)
    print(f"å¥–åŠ±: {reward_color}{reward:+.2f}{Colors.END}\n")
    
    # æ‰“å°å›å¤ï¼ˆæ ¼å¼åŒ–ï¼‰
    print(f"{Colors.CYAN}æ¨¡å‹å›å¤:{Colors.END}")
    print(f"{Colors.CYAN}â”Œ{'â”€'*78}â”{Colors.END}")
    
    lines = response.strip().split('\n')
    for i, line in enumerate(lines[:12]):  # æœ€å¤šæ˜¾ç¤º12è¡Œ
        if len(line) > 76:
            line = line[:73] + "..."
        print(f"{Colors.CYAN}â”‚{Colors.END} {line.ljust(76)} {Colors.CYAN}â”‚{Colors.END}")
    
    if len(lines) > 12:
        print(f"{Colors.CYAN}â”‚{Colors.END} ... ({len(lines)-12} è¡Œçœç•¥)".ljust(77) + f"{Colors.CYAN}â”‚{Colors.END}")
    
    print(f"{Colors.CYAN}â””{'â”€'*78}â”˜{Colors.END}\n")

def display_training_step(step, metrics, samples):
    """æ˜¾ç¤ºå®Œæ•´çš„è®­ç»ƒæ­¥éª¤ä¿¡æ¯"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    
    # æ¸…ç©ºå±å¹•ï¼ˆå¯é€‰ï¼‰
    # os.system('clear' if os.name != 'nt' else 'cls')
    
    # å¤§æ ‡é¢˜
    print_header(f"ğŸš€ è®­ç»ƒæ­¥éª¤ {step} | {timestamp}")
    
    # çŠ¶æ€åˆ†æ
    print_section("ğŸ” çŠ¶æ€è¯Šæ–­")
    analysis = analyze_metrics(metrics)
    for text, color, detail in analysis:
        print(f"  {color}â— {text}: {detail}{Colors.END}")
    print()
    
    # æŒ‡æ ‡ä»ªè¡¨ç›˜
    print_metrics_dashboard(metrics)
    
    # æ ·æœ¬åˆ†æ
    if samples:
        print_section("ğŸ’¬ æ¨¡å‹å›å¤æ ·ä¾‹")
        for i, sample in enumerate(samples[:3]):  # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬
            print_sample_analysis(sample, i)
    
    print(f"{Colors.HEADER}{'='*80}{Colors.END}\n")

def monitor_training():
    """ç›‘æ§è®­ç»ƒè¿‡ç¨‹"""
    metrics_file = Path("/home/uincy/projects/mywsl/ppo_math/outputs/ppo_math/metrics.jsonl")
    samples_file = Path("/home/uincy/projects/mywsl/ppo_math/outputs/ppo_math/samples.jsonl")
    
    if not metrics_file.exists():
        print(f"{Colors.RED}é”™è¯¯: æ‰¾ä¸åˆ° metrics.jsonl{Colors.END}")
        return
    
    print(f"{Colors.GREEN}å¼€å§‹ç›‘æ§è®­ç»ƒ...{Colors.END}")
    print(f"ç›‘æ§æ–‡ä»¶: {metrics_file}")
    print(f"æ ·æœ¬æ–‡ä»¶: {samples_file}")
    print(f"{Colors.YELLOW}æç¤º: æŒ‰ Ctrl+C é€€å‡º{Colors.END}\n")
    
    last_step = -1
    
    try:
        while True:
            # è¯»å–æœ€æ–°çš„æŒ‡æ ‡
            if metrics_file.exists():
                with open(metrics_file, 'r') as f:
                    lines = f.readlines()
                    if lines:
                        # è·å–æœ€åä¸€ä¸ªstepçš„æŒ‡æ ‡
                        for line in reversed(lines):
                            try:
                                metrics = json.loads(line)
                                step = metrics.get("step", 0)
                                
                                # åªå¤„ç†æ¯10ä¸ªstep
                                if step % 10 == 0 and step != last_step:
                                    # è¯»å–å¯¹åº”çš„æ ·æœ¬
                                    samples = []
                                    if samples_file.exists():
                                        with open(samples_file, 'r') as sf:
                                            for sline in sf:
                                                try:
                                                    sample = json.loads(sline)
                                                    if sample.get("step") == step:
                                                        samples.append(sample)
                                                except:
                                                    pass
                                    
                                    # æ˜¾ç¤º
                                    display_training_step(step, metrics, samples)
                                    last_step = step
                                break
                            except:
                                continue
            
            time.sleep(2)  # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡
            
    except KeyboardInterrupt:
        print(f"\n{Colors.YELLOW}ç›‘æ§å·²åœæ­¢{Colors.END}")

def show_historical_data():
    """æ˜¾ç¤ºå†å²è®­ç»ƒæ•°æ®"""
    metrics_file = Path("/home/uincy/projects/mywsl/ppo_math/outputs/ppo_math/metrics.jsonl")
    samples_file = Path("/home/uincy/projects/mywsl/ppo_math/outputs/ppo_math/samples.jsonl")
    
    if not metrics_file.exists():
        print(f"{Colors.RED}é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®{Colors.END}")
        return
    
    # è¯»å–æ‰€æœ‰æŒ‡æ ‡
    all_metrics = []
    with open(metrics_file, 'r') as f:
        for line in f:
            try:
                metrics = json.loads(line)
                all_metrics.append(metrics)
            except:
                pass
    
    # åªæ˜¾ç¤ºæ¯10ä¸ªstepçš„æ•°æ®
    for metrics in all_metrics:
        step = metrics.get("step", 0)
        if step % 10 == 0:
            # è¯»å–å¯¹åº”çš„æ ·æœ¬
            samples = []
            if samples_file.exists():
                with open(samples_file, 'r') as sf:
                    for line in sf:
                        try:
                            sample = json.loads(line)
                            if sample.get("step") == step:
                                samples.append(sample)
                        except:
                            pass
            
            display_training_step(step, metrics, samples)
            
            # è¯¢é—®æ˜¯å¦ç»§ç»­
            if step < all_metrics[-1].get("step", 0):
                try:
                    input(f"{Colors.YELLOW}æŒ‰ Enter æŸ¥çœ‹ä¸‹ä¸€æ­¥ï¼Œæˆ–æŒ‰ Ctrl+C é€€å‡º...{Colors.END}")
                except KeyboardInterrupt:
                    print(f"\n{Colors.YELLOW}å·²é€€å‡º{Colors.END}")
                    break

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--history":
        show_historical_data()
    else:
        monitor_training()
